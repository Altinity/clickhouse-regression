version: '2.3'

volumes:
  data1-1:
  parquet-files:

services:
  spark-iceberg:
    image: tabulario/spark-iceberg
    container_name: spark-iceberg
    build: spark/
    depends_on:
      - rest
      - minio
    environment:
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
      - AWS_REGION=us-east-1
    ports:
      - 8080:8080
      - 10000:10000
      - 10001:10001
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  rest:
    image: tabulario/iceberg-rest
    ports:
      - 8182:8181
    environment:
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3://warehouse/data/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000

  zookeeper1:
    extends:
      file: ../../docker-compose/zookeeper-service.yml
      service: zookeeper-alone

  keeper1:
    extends:
      file: ../../docker-compose/keeper-service.yml
      service: keeper1

  clickhouse-antalya:
    extends:
      file: clickhouse-service.yml
      service: clickhouse
    hostname: clickhouse-antalya
    volumes:
      - "${CLICKHOUSE_TESTS_DIR}/configs/antalya/remote_servers.xml:/etc/clickhouse-server/config.d/remote.xml"
      - "${CLICKHOUSE_TESTS_DIR}/_instances/antalya/database/:/var/lib/clickhouse/"
      - "${CLICKHOUSE_TESTS_DIR}/_instances/antalya:/var/log/clickhouse-server"

  clickhouse-swarm-1:
    extends:
      file: clickhouse-service.yml
      service: clickhouse
    hostname: clickhouse-swarm-1
    volumes:
      - "${CLICKHOUSE_TESTS_DIR}/configs/swarm/remote_servers.xml:/etc/clickhouse-server/config.d/remote.xml"
      - "${CLICKHOUSE_TESTS_DIR}/_instances/swarm-1/database/:/var/lib/clickhouse/"
      - "${CLICKHOUSE_TESTS_DIR}/_instances/swarm-1:/var/log/clickhouse-server"

  clickhouse-swarm-2:
    extends:
      file: clickhouse-service.yml
      service: clickhouse
    hostname: clickhouse-swarm-2
    volumes:
      - "${CLICKHOUSE_TESTS_DIR}/configs/swarm/remote_servers.xml:/etc/clickhouse-server/config.d/remote.xml"
      - "${CLICKHOUSE_TESTS_DIR}/_instances/swarm-2/database/:/var/lib/clickhouse/"
      - "${CLICKHOUSE_TESTS_DIR}/_instances/swarm-2:/var/log/clickhouse-server"

  clickhouse1:
    extends:
      file: clickhouse-service.yml
      service: clickhouse
    hostname: clickhouse1
    volumes:
      - "${CLICKHOUSE_TESTS_DIR}/_instances/clickhouse1/database/:/var/lib/clickhouse/"
      - "${CLICKHOUSE_TESTS_DIR}/_instances/clickhouse1/logs/:/var/log/clickhouse-server/"
      - "${CLICKHOUSE_TESTS_DIR}/configs/clickhouse1/config.d/macros.xml:/etc/clickhouse-server/config.d/macros.xml"
      - "${CLICKHOUSE_TESTS_DIR}/_instances/share:/share"
      - parquet-files:/parquet-files
    depends_on:
      zookeeper1:
        condition: service_healthy

  clickhouse2:
    extends:
      file: clickhouse-service.yml
      service: clickhouse
    hostname: clickhouse2
    volumes:
      - "${CLICKHOUSE_TESTS_DIR}/_instances/clickhouse2/database/:/var/lib/clickhouse/"
      - "${CLICKHOUSE_TESTS_DIR}/_instances/clickhouse2/logs/:/var/log/clickhouse-server/"
      - "${CLICKHOUSE_TESTS_DIR}/configs/clickhouse2/config.d/macros.xml:/etc/clickhouse-server/config.d/macros.xml"
      - parquet-files:/parquet-files
    depends_on:
      zookeeper1:
        condition: service_healthy

  clickhouse3:
    extends:
      file: clickhouse-service.yml
      service: clickhouse
    hostname: clickhouse3
    volumes:
      - "${CLICKHOUSE_TESTS_DIR}/_instances/clickhouse3/database/:/var/lib/clickhouse/"
      - "${CLICKHOUSE_TESTS_DIR}/_instances/clickhouse3/logs/:/var/log/clickhouse-server/"
      - "${CLICKHOUSE_TESTS_DIR}/configs/clickhouse3/config.d/macros.xml:/etc/clickhouse-server/config.d/macros.xml"
      - parquet-files:/parquet-files
    depends_on:
      zookeeper1:
        condition: service_healthy

  proxy1:
    image: altinityinfra/s3-proxy:265-ea7abf434d5f99618d1e188b8284cf43835724f3
    ports:
      - "8082"
      - "80"
      - "443"

  proxy2:
    image: altinityinfra/s3-proxy:265-ea7abf434d5f99618d1e188b8284cf43835724f3
    ports:
      - "8082"
      - "80"
      - "443"

  resolver:
    image: altinityinfra/python-bottle:265-ea7abf434d5f99618d1e188b8284cf43835724f3
    ports:
      - "8082"
    tty: true
    init: true
    depends_on:
      - proxy1
      - proxy2

  minio:
    extends:
      file: minio-service.yml
      service: minio

  minio-client:
    extends:
      file: minio-client.yml
      service: minio-client
    depends_on:
      minio:
        condition: service_healthy

  aws:
    extends:
      file: aws-client.yml
      service: aws

  mysql1:
    extends:
      file: mysql-service.yml
      service: mysql
    hostname: mysql1
    volumes:
      - "${CLICKHOUSE_TESTS_DIR}/_instances/mysql1/database:/var/lib/mysql"

  postgres1:
    extends:
      file: postgres-service.yml
      service: postgres
    hostname: postgres1
    volumes:
      - "${CLICKHOUSE_TESTS_DIR}/_instances/postgres1/database:/var/lib/postgres"

  bash-tools:
    extends:
      file: bash-tools.yml
      service: bash-tools
    hostname: bash-tools
    volumes:
      - "${CLICKHOUSE_TESTS_DIR}/_instances/share:/share"

  parquetify:
    extends:
      file: parquetify-service.yml
      service: parquetify
    volumes:
      - parquet-files:/parquet-files

  # dummy service which does nothing, but allows to postpone
  # 'docker-compose up -d' till all dependencies will go healthy
  all_services_ready:
    image: hello-world
    depends_on:
      clickhouse1:
        condition: service_healthy
      clickhouse2:
        condition: service_healthy
      clickhouse3:
        condition: service_healthy
      clickhouse-antalya:
        condition: service_healthy
      clickhouse-swarm-1:
        condition: service_healthy
      clickhouse-swarm-2:
        condition: service_healthy
      zookeeper1:
        condition: service_healthy
      keeper1:
        condition: service_healthy
      mysql1:
        condition: service_healthy
      postgres1:
        condition: service_healthy
